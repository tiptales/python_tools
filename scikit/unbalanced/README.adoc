= ANOMALY DETECTION
Procedure for anomaly detection with quantitative and qualitative predictors

== FRAMEWORK

The procedure follows the general framework mentioned in link:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3946903/[ANALYSIS OF SAMPLING TECHNIQUES FOR IMBALANCED DATA: AN N=648 ADNI STUDY]

- Find a consistent statistical model

- Find a robust estimator

- Resample the models

Note: all scripts assume the data have a definite type

===== The final model will be one of the subset in the following combinatorics

- Set of feature selection algorithms:

    FS = {Pairwise Wilcon Test, Kruskal Test, Chi-Square Test, Two way Anova (III), Relief-F, Gini Index, Information Gain, SLR+SS}

- Set of class-imbalance handling approaches:

    CIS = {Different types and rates of data re-sampling techniques}

- Set of classification algorithms:

    MS = {Random Forest, Support Vector Machine}

==== An ensemble system is defined as follows:

E= {f, c, m}, where f ∈ FS,  c ∈ CIS, and m ∈ MS


[filetree]
* FS: features_selection/
** features/
** outliers/
* MS: estimation/
** baseline/
** ensemble/
** gridsearch/
* CIS: resampling/
*** sample/



==== FEATURES SELECTION

* Functions for statistical modeling and reduction

This step searches for a valid statistical model. The general hypothesis for retaining a variable is that this variable should allow to discriminate between the normal event group and the exceptional event group.
As there is no initial assumption on how the data are distributed, parametric and non parametric tests are performed.
Different variables can be retained for different models depending on the results of the tests
i.e. Anova valid variables will be privileged for Logistic Regression, Chi Square valid variables will be privileged for Random Forest.

==== RESMAPLING
* Tests for different resampling methods, regarding the selected variables


==== MODEL SELECTION
* Tests for different classifiers

The baseline is a xgboost model.
Ensemble learning and specific classifiers are tested
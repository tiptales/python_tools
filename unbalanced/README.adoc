= ANOMALY DETECTION
Procedure for anomaly detection with quantitative and qualitative predictors

== FRAMEWORK

The procedure follows the general framework mentioned in link:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3946903/[ANALYSIS OF SAMPLING TECHNIQUES FOR IMBALANCED DATA: AN N=648 ADNI STUDY]

- Find a consistent statistical model

- Find a robust estimator

- Resample the models

Note: all scripts assume the data have a definite type

===== The final model will be one of the subset in the following combinatorics

- Set of feature selection algorithms:

    FS = {Pairwise Wilcon Test, Kruskal Test, Chi-Square Test, Two way Anova (III), Relief-F, Gini Index, Information Gain, SLR+SS}

- Set of class-imbalance handling approaches:

    CIS = {Different types and rates of data re-sampling techniques}

- Set of classification algorithms:

    MS = {Random Forest, Support Vector Machine}

==== An ensemble system is defined as follows:

E= {f, c, m}, where f ∈ FS,  c ∈ CIS, and m ∈ MS


[filetree]
* FS: features_engineering/
** covariates/
** outliers/
* MS: estimation/
** baseline/
** models/
** gridsearchORspecifics[TODO]/
* CIS: resampling/
*** sample/



==== FEATURES ENGINEERING

* Functions for statistical modeling and reduction

This step searches for a valid statistical model. The general hypothesis for retaining a variable is that this variable should allow to discriminate between the normal event group and the exceptional event group.
As there is no initial assumption on how the data are distributed, parametric and non parametric tests are performed.
Different variables can be retained for different models depending on the results of the tests
i.e. Anova valid variables will be privileged for Logistic Regression, Chi Square valid variables will be privileged for Random Forest.

==== RESAMPLING
* Tests for different resampling methods, regarding the selected variables


==== MODEL SELECTION
* Tests for different classifiers

- The baseline is a xgboost model. It is used to evaluate the performance augmentation of different features selections methods (different sets of variables) from previous step.
- The models are an ensemble of simple classifiers evaluating parametric and non parametric models.

== Examples

- Covariate analysis report

    python features_engineering/covariates.py --infile data/bank/bank.csv --Y y --method pearson --Y2 job --dim 4

- Make baseline

    python estimation/baseline.py --infile data/bank/bank.csv --Y y --index 0 --k 10 --scale scaler --nan mean


- Test Classifiers

    python estimation/models.py --infile data/bank/bank.csv --Y y

- Resampling test

    python resampling/sample.py --model rf --infile data/bank/bank.csv --Y y --majority no --minority yes



